{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from   datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from   torch.utils.data import DataLoader, TensorDataset\n",
    "import ot\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from matplotlib.collections import LineCollection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1. Experimental Parameters\n",
    "\n",
    "\n",
    "\n",
    " All key parameters for the experiment are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Parameters ---\n",
    "N_TRAIN = 1000            # Number of samples to train on\n",
    "N_TRUTH = 50000          # Number of \"ground truth\" samples for evaluation\n",
    "N_GEN = 20000            # Number of samples to generate from the model\n",
    "\n",
    "# --- Model & Schedule Parameters ---\n",
    "T = 200                  # Number of diffusion steps\n",
    "SCHEDULE_S = 0.008       # Offset for cosine schedule\n",
    "\n",
    "# --- Training Parameters ---\n",
    "N_EPOCHS = 1000          # Increase epochs for the small dataset\n",
    "BATCH_SIZE = 32          # Batch size (must be <= N_TRAIN)\n",
    "LR = 1e-3                # Learning rate\n",
    "\n",
    "# --- Trajectory/Misc Parameters ---\n",
    "START_NOISE_VEC = torch.tensor([[-0.4, -0.2]]) # Fixed start for trajectory plots\n",
    "N_TRAJ_RUNS = 2          # Number of stochastic trajectories to plot\n",
    "\n",
    "# --- Output ---\n",
    "run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir    = f\"run_{run_timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Created output directory: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2. Phase 1: Data Generation\n",
    "\n",
    "\n",
    "\n",
    " Define the 3-circle generator and create two distinct datasets:\n",
    "\n",
    " 1.  `X_truth_tensor`: A large (50k) set representing the \"ground truth\" distribution. Used *only* for evaluation.\n",
    "\n",
    " 2.  `X_train_tensor`: A tiny (100) set for the model to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the three circles dataset generator ---\n",
    "circles = {\n",
    "    1: (np.array([ 2.0,  0.0]), 1.0),   # circle 1 at (2,0), radius 1\n",
    "    2: (np.array([-2.0, -3.0]), 2.0),   # circle 2 at (-2,-3), radius 2\n",
    "    3: (np.array([-3.0,  4.0]), 3.0)    # circle 3 at (-3,4), radius 3\n",
    "}\n",
    "choices = [1, 2, 2, 3, 3, 3] # weighting 1:2:3\n",
    "\n",
    "def G():\n",
    "    \"\"\"Generate one uniformly random point on the perimeter of a weighted circle.\"\"\"\n",
    "    cid = np.random.choice(choices)\n",
    "    theta = np.random.uniform(0, 2*np.pi)\n",
    "    center, r = circles[cid]\n",
    "    x = center[0] + r * np.cos(theta)\n",
    "    y = center[1] + r * np.sin(theta)\n",
    "    return (x, y), cid\n",
    "\n",
    "def generate_data(N):\n",
    "    \"\"\"Generates N samples from the 3-circles generator.\"\"\"\n",
    "    samples_list, labels_list = zip(*(G() for _ in range(N)))\n",
    "    X_np = np.array(samples_list)\n",
    "    X_tensor = torch.tensor(X_np, dtype=torch.float32)\n",
    "    return X_np, X_tensor, np.array(labels_list)\n",
    "\n",
    "# --- Generate the two datasets ---\n",
    "X_truth_np, X_truth_tensor, _ = generate_data(N_TRUTH)\n",
    "X_train_np, X_train_tensor, _ = generate_data(N_TRAIN)\n",
    "\n",
    "print(f\"Ground Truth Dataset Shape: {X_truth_tensor.shape}\")\n",
    "print(f\"Training Dataset Shape:     {X_train_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize the source and training data ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "fig.suptitle(\"Input Data\")\n",
    "\n",
    "# Plot 1: Ground Truth Distribution (Histogram)\n",
    "ax1.hist2d(X_truth_np[:,0], X_truth_np[:,1], bins=150, density=True, cmap='viridis')\n",
    "ax1.set_title(f\"Ground Truth Distribution (N={N_TRUTH})\")\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Plot 2: Training Data (Scatter)\n",
    "ax2.scatter(X_train_np[:,0], X_train_np[:,1], s=20, alpha=0.8, c='r', label='Training Points')\n",
    "ax2.set_title(f\"Training Samples (N={N_TRAIN})\")\n",
    "ax2.set_aspect('equal')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3. Model & Schedule Definitions\n",
    "\n",
    "\n",
    "\n",
    " Define the `DenoiseMLP` model and the noise schedule functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(T, s=0.008):\n",
    "    \"\"\"Cosine schedule from https://arxiv.org/abs/2102.09672\"\"\"\n",
    "    steps = T + 1\n",
    "    x = torch.linspace(0, T, steps)\n",
    "    alphas_cumprod = torch.cos(((x / T) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clamp(betas, 0.0001, 0.9999).float()\n",
    "\n",
    "def get_schedules(schedule_type, T):\n",
    "    \"\"\"Returns beta, alpha, and alpha_bar for a given schedule.\"\"\"\n",
    "    if schedule_type == 'linear':\n",
    "        beta = torch.linspace(1e-4, 0.02, T)\n",
    "    elif schedule_type == 'cosine':\n",
    "        beta = cosine_beta_schedule(T, s=SCHEDULE_S)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown schedule_type: {schedule_type}\")\n",
    "        \n",
    "    alpha = 1 - beta\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "    return beta, alpha, alpha_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoiseMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2+1, 128)  # +1 for time embedding\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "    \n",
    "    def forward(self, x, t): # t is normalized time\n",
    "        # Normalize t\n",
    "        t_norm = t / T\n",
    "        t_embed = t_norm.unsqueeze(-1)\n",
    "        h = torch.cat([x, t_embed], dim=1)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc3(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 4. Sampling & Evaluation Functions\n",
    "\n",
    "\n",
    "\n",
    " Define helper functions to:\n",
    "\n",
    " 1.  `sample`: Generate new data from a trained model.\n",
    "\n",
    " 2.  `evaluate_distributions`: Create all plots and calculate all metrics.\n",
    "\n",
    " 3.  `plot_stochastic_trajectories`: Plot the N-run trajectory experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, n_samples, T, alpha, alpha_bar, beta):\n",
    "    \"\"\"Generates n_samples from the trained model.\"\"\"\n",
    "    x = torch.randn(n_samples, 2)  # start from pure noise\n",
    "    snapshots = {}\n",
    "    \n",
    "    num_snapshots = min(9, T)\n",
    "    snapshot_timesteps = np.unique(np.round(np.linspace(0, T - 1, num_snapshots)).astype(int)).tolist()    \n",
    "    snapshot_timesteps = snapshot_timesteps[::-1]\n",
    "    \n",
    "    model.eval()\n",
    "    for t in reversed(range(T)):\n",
    "        t_batch = torch.full((n_samples,), t, dtype=torch.float32)\n",
    "        predicted_noise = model(x, t_batch)\n",
    "        \n",
    "        alpha_t = alpha[t]\n",
    "        alpha_bar_t = alpha_bar[t]\n",
    "        beta_t = beta[t]\n",
    "        \n",
    "        x = 1/torch.sqrt(alpha_t) * (x - (1-alpha_t)/torch.sqrt(1-alpha_bar_t)*predicted_noise)\n",
    "        \n",
    "        if t > 0:\n",
    "            x += torch.sqrt(beta_t) * torch.randn_like(x)\n",
    "\n",
    "        if t in snapshot_timesteps:\n",
    "            snapshots[int(t)] = x.clone().detach().cpu()\n",
    "    \n",
    "    # Plot snapshots\n",
    "    if len(snapshots) > 0:\n",
    "        times = [t for t in snapshot_timesteps if t in snapshots]\n",
    "        n = len(times)\n",
    "        ncols = int(np.ceil(np.sqrt(n)))\n",
    "        nrows = int(np.ceil(n / ncols))\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(4 * ncols, 3 * nrows))\n",
    "        axes_list = axes.ravel() if isinstance(axes, np.ndarray) else [axes]\n",
    "\n",
    "        for idx, t in enumerate(times):\n",
    "            ax  = axes_list[idx]\n",
    "            x_t = snapshots[t]\n",
    "            ax.hist2d(x_t[:, 0].numpy(), x_t[:, 1].numpy(), bins=100, density=True, cmap='viridis')\n",
    "            ax.set_title(f'Timestep {t}')\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            ax.set_aspect('equal')\n",
    "\n",
    "        for ax in axes_list[n:]:\n",
    "            ax.axis('off')\n",
    "            \n",
    "        fig.set_dpi(150)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return x.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_distributions(X_truth_tensor, samples_gen_tensor, X_train_tensor, schedule_type):\n",
    "    \"\"\"Compares generated data to truth data and plots results.\"\"\"\n",
    "    \n",
    "    print(\"--- 1. Visual Check: Manifold vs. Memorization ---\")\n",
    "    \n",
    "    # --- Plot 1: Manifold vs. Memorization (Item 8) ---\n",
    "    X_train_np = X_train_tensor.cpu().numpy()\n",
    "    samples_gen_np = samples_gen_tensor.cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(samples_gen_np[:, 0], samples_gen_np[:, 1], s=1, alpha=0.1, label=f\"Generated (N={N_GEN})\")\n",
    "    plt.scatter(X_train_np[:, 0], X_train_np[:, 1], s=40, facecolors='none', edgecolors='r', lw=1.5, label=f\"Training (N={N_TRAIN})\")\n",
    "    plt.title(f\"Manifold vs. Memorization ({schedule_type.capitalize()} Schedule)\")\n",
    "    plt.legend()\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.savefig(os.path.join(output_dir, f\"manifold_plot_{schedule_type}.png\"), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- 2. Quantitative Check: Metrics vs. Ground Truth ---\")\n",
    "    \n",
    "    # --- Calculations (Item 9) ---\n",
    "    X_truth_np = X_truth_tensor.cpu().numpy()\n",
    "    \n",
    "    # We must compare equal numbers of samples for metrics\n",
    "    n_compare = min(len(X_truth_np), len(samples_gen_np))\n",
    "    \n",
    "    # Get random subset of truth data\n",
    "    truth_indices = np.random.choice(len(X_truth_np), n_compare, replace=False)\n",
    "    X_truth_subset = X_truth_np[truth_indices]\n",
    "    \n",
    "    # Get random subset of generated data\n",
    "    gen_indices = np.random.choice(len(samples_gen_np), n_compare, replace=False)\n",
    "    samples_gen_subset = samples_gen_np[gen_indices]\n",
    "    \n",
    "    # A) Sliced Wasserstein Distance\n",
    "    print(f\"Calculating SWD between {n_compare} truth and {n_compare} generated samples...\")\n",
    "    swd = ot.sliced_wasserstein_distance(X_truth_subset, samples_gen_subset, n_projections=100)\n",
    "    \n",
    "    # B) KL & JS Divergence (requires binning)\n",
    "    n_bins = 100\n",
    "    x_min = min(X_truth_np[:,0].min(), samples_gen_np[:,0].min()) - 0.1\n",
    "    x_max = max(X_truth_np[:,0].max(), samples_gen_np[:,0].max()) + 0.1\n",
    "    y_min = min(X_truth_np[:,1].min(), samples_gen_np[:,1].min()) - 0.1\n",
    "    y_max = max(X_truth_np[:,1].max(), samples_gen_np[:,1].max()) + 0.1\n",
    "    hist_range = [[x_min, x_max], [y_min, y_max]]\n",
    "\n",
    "    # Use the full datasets for histogram probability distributions\n",
    "    real_hist_counts, _, _ = np.histogram2d(X_truth_np[:,0], X_truth_np[:,1], bins=n_bins, range=hist_range)\n",
    "    generated_hist_counts, _, _ = np.histogram2d(samples_gen_np[:,0], samples_gen_np[:,1], bins=n_bins, range=hist_range)\n",
    "\n",
    "    real_hist_probs = real_hist_counts / real_hist_counts.sum()\n",
    "    fake_hist_probs = generated_hist_counts / generated_hist_counts.sum()\n",
    "\n",
    "    Q = real_hist_probs.flatten() + 1e-10  # Ground Truth\n",
    "    P = fake_hist_probs.flatten() + 1e-10  # Generated\n",
    "    \n",
    "\n",
    "    kl_div = entropy(P, Q)\n",
    "    kl_div2 = entropy(Q, P)\n",
    "    js_div = jensenshannon(P, Q, base=2)**2\n",
    "    \n",
    "    print(f\"  Sliced Wasserstein Distance: {swd:.6f}\")\n",
    "    print(f\"  KL Divergence (Gen || Truth): {kl_div:.6f}\")\n",
    "    print(f\"  KL Divergence (Truth || Gen): {kl_div2:.6f}\")\n",
    "    print(f\"  JS Divergence: {js_div:.6f}\")\n",
    "    \n",
    "    metrics = {'swd': swd, 'kl': kl_div, 'kl 2':kl_div2,'jsd': js_div}\n",
    "\n",
    "    # --- Plot 2: Histogram Comparison (Item 9) ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f\"Distribution Comparison ({schedule_type.capitalize()} Schedule)\")\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    ax1.imshow(real_hist_probs.T, origin='lower', extent=(x_min, x_max, y_min, y_max), aspect='auto', cmap='viridis')\n",
    "    ax1.set_title(f'Ground Truth (N={N_TRUTH})')\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    im = ax2.imshow(fake_hist_probs.T, origin='lower', aspect='auto', extent=[x_min, x_max, y_min, y_max], cmap='viridis')\n",
    "    ax2.set_title(f'Generated (N={N_GEN})')\n",
    "    \n",
    "    fig.colorbar(im, ax=axes.ravel().tolist(), label='Probability Density')\n",
    "    plt.savefig(os.path.join(output_dir, f\"hist_compare_{schedule_type}.png\"), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_single_trajectory(model, start_noise, T, alpha, alpha_bar, beta):\n",
    "    \"\"\"Helper for trajectory plotting.\"\"\"\n",
    "    model.eval()\n",
    "    x = start_noise.clone()\n",
    "    trajectory = [x.clone().detach().cpu()]\n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "        t_batch = torch.full((1,), t, dtype=torch.float32)\n",
    "        predicted_noise = model(x, t_batch)\n",
    "        alpha_t = alpha[t]\n",
    "        alpha_bar_t = alpha_bar[t]\n",
    "        beta_t = beta[t]\n",
    "        \n",
    "        mu_tilde = 1 / torch.sqrt(alpha_t) * (x - (1 - alpha_t) / torch.sqrt(1 - alpha_bar_t) * predicted_noise)\n",
    "        \n",
    "        if t > 0:\n",
    "            z = torch.randn_like(x)\n",
    "            x = mu_tilde + torch.sqrt(beta_t) * z\n",
    "        else:\n",
    "            x = mu_tilde # Final step is deterministic\n",
    "            \n",
    "        trajectory.append(x.clone().detach().cpu())\n",
    "    \n",
    "    return torch.cat(trajectory, dim=0)\n",
    "\n",
    "def plot_stochastic_trajectories(model, T, alpha, alpha_bar, beta, X_truth_tensor, start_noise_vec, n_runs, schedule_type):\n",
    "    \"\"\"Plots N_RUNS trajectories from the same starting noise.\"\"\"\n",
    "    \n",
    "    print(f\"\\n--- 3. Stochastic Trajectory Plot (Item 11) ---\")\n",
    "    \n",
    "    all_trajectories = []\n",
    "    for _ in range(n_runs):\n",
    "        trajectory = generate_single_trajectory(model, start_noise_vec, T, alpha, alpha_bar, beta)\n",
    "        all_trajectories.append(trajectory)\n",
    "\n",
    "    fig, axes = plt.subplots(n_runs, 1, figsize=(8, 8 * n_runs), sharex=True, sharey=True)\n",
    "    if n_runs == 1: axes = [axes] # Make it iterable\n",
    "        \n",
    "    fig.suptitle(f\"Stochastic Trajectories from Single Start ({schedule_type.capitalize()})\", fontsize=16)\n",
    "\n",
    "    X_truth_np = X_truth_tensor.cpu().numpy()\n",
    "    x_min, x_max = X_truth_np[:, 0].min() - 1, X_truth_np[:, 0].max() + 1\n",
    "    y_min, y_max = X_truth_np[:, 1].min() - 1, X_truth_np[:, 1].max() + 1\n",
    "    plot_range = [[x_min, x_max], [y_min, y_max]]\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        ax = axes[i]\n",
    "        numpy_trajectory = all_trajectories[i].numpy()\n",
    "\n",
    "        # Plot background\n",
    "        ax.hist2d(X_truth_np[:, 0], X_truth_np[:, 1], bins=100, cmap='Greys', density=True, alpha=0.6, range=plot_range)\n",
    "\n",
    "        # Plot trajectory line\n",
    "        points = numpy_trajectory\n",
    "        segments = np.stack([points[:-1], points[1:]], axis=1)\n",
    "        lc = LineCollection(segments, cmap='RdYlBu', norm=plt.Normalize(0, len(segments)))\n",
    "        lc.set_array(np.arange(len(segments)))\n",
    "        lc.set_linewidth(1)\n",
    "        ax.add_collection(lc)\n",
    "        \n",
    "        # Plot start and end points\n",
    "        ax.plot(numpy_trajectory[0, 0], numpy_trajectory[0, 1], 'o', c='b', markersize=8, label='Start (Noise)')\n",
    "        ax.plot(numpy_trajectory[-1, 0], numpy_trajectory[-1, 1], 'x', c='r', markersize=10, mew=2, label='End (Generated)')\n",
    "\n",
    "        ax.set_title(f\"Run {i+1}\")\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        if i == 0: ax.legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(os.path.join(output_dir, f\"trajectories_{schedule_type}.png\"), dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 5. Main Experiment Loop\n",
    "\n",
    "\n",
    "\n",
    " This function wraps the entire Phase 2 (Training) and Phase 3 (Evaluation)\n",
    "\n",
    " logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(schedule_type, X_train, X_truth, T, n_epochs, batch_size, lr):\n",
    "    \"\"\"\n",
    "    Runs the full training and evaluation pipeline for a given schedule.\n",
    "    \"\"\"\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"  STARTING EXPERIMENT: {schedule_type.upper()} SCHEDULE\")\n",
    "    print(f\"  Training on {len(X_train)} samples for {n_epochs} epochs.\")\n",
    "    print(f\"  Evaluating against {len(X_truth)} ground truth samples.\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # --- 1. Setup Schedules & Model ---\n",
    "    beta, alpha, alpha_bar = get_schedules(schedule_type, T)\n",
    "    \n",
    "    model = DenoiseMLP()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    dataset = TensorDataset(X_train)\n",
    "    dataLoader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # --- 2. Training Loop (Phase 2) ---\n",
    "    epoch_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        batch_losses = []\n",
    "        for (batch_x0,) in dataLoader:\n",
    "            current_batch_size = batch_x0.shape[0]\n",
    "            \n",
    "            t = torch.randint(0, T, (current_batch_size,)).to(batch_x0.device)\n",
    "            \n",
    "            noise = torch.randn_like(batch_x0)\n",
    "            alpha_bar_t = alpha_bar[t].unsqueeze(-1)\n",
    "            \n",
    "            x_t = torch.sqrt(alpha_bar_t) * batch_x0 + torch.sqrt(1 - alpha_bar_t) * noise\n",
    "            \n",
    "            noise_pred = model(x_t, t.float())\n",
    "            \n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "            \n",
    "        avg_epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "\n",
    "        if (epoch+1) % (n_epochs // 5) == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_epoch_loss:.6f}\")\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(n_epochs), epoch_losses)\n",
    "    plt.title(f'Training Loss ({schedule_type.capitalize()} Schedule)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error (MSE) Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, f\"training_loss_{schedule_type}.png\"), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # --- 3. Evaluation (Phase 3) ---\n",
    "    print(\"\\n--- Generating Samples ---\")\n",
    "    samples_gen = sample(model, N_GEN, T, alpha, alpha_bar, beta)\n",
    "    \n",
    "    print(\"\\n--- Evaluating Distributions ---\")\n",
    "    metrics = evaluate_distributions(X_truth, samples_gen, X_train, schedule_type)\n",
    "    \n",
    "    plot_stochastic_trajectories(model, T, alpha, alpha_bar, beta, X_truth, START_NOISE_VEC, N_TRAJ_RUNS, schedule_type)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"  FINISHED EXPERIMENT: {schedule_type.upper()}\")\n",
    "    print(\"=\"*50 + \"\\n\\n\")\n",
    "    \n",
    "    return model, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6. Run Experiments & Compare Results (Phase 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "# --- Experiment 1: Linear Schedule ---\n",
    "lin_model, lin_metrics = train_and_evaluate(\n",
    "    schedule_type='linear',\n",
    "    X_train=X_train_tensor,\n",
    "    X_truth=X_truth_tensor,\n",
    "    T=T, n_epochs=N_EPOCHS, batch_size=BATCH_SIZE, lr=LR\n",
    ")\n",
    "all_results['linear'] = lin_metrics\n",
    "\n",
    "\n",
    "# --- Experiment 2: Cosine Schedule ---\n",
    "cos_model, cos_metrics = train_and_evaluate(\n",
    "    schedule_type='cosine',\n",
    "    X_train=X_train_tensor,\n",
    "    X_truth=X_truth_tensor,\n",
    "    T=T, n_epochs=N_EPOCHS, batch_size=BATCH_SIZE, lr=LR\n",
    ")\n",
    "all_results['cosine'] = cos_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 7. Final Comparison (Item 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- FINAL METRIC COMPARISON ---\")\n",
    "\n",
    "labels = list(all_results['linear'].keys())\n",
    "linear_scores = list(all_results['linear'].values())\n",
    "cosine_scores = list(all_results['cosine'].values())\n",
    "\n",
    "print(f\"Metric        | Linear        | Cosine\")\n",
    "print(f\"--------------|---------------|--------------\")\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"{label.upper():<14}| {linear_scores[i]:<14.6f}| {cosine_scores[i]:<14.6f}\")\n",
    "\n",
    "# --- Final Bar Chart ---\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "rects1 = ax.bar(x - width/2, linear_scores, width, label='Linear')\n",
    "rects2 = ax.bar(x + width/2, cosine_scores, width, label='Cosine')\n",
    "\n",
    "ax.set_ylabel('Score (Lower is Better)')\n",
    "ax.set_title('Final Metric Comparison: Linear vs. Cosine Schedule')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3, fmt='%.4f')\n",
    "ax.bar_label(rects2, padding=3, fmt='%.4f')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"final_comparison.png\"), dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
